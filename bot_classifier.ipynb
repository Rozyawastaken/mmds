{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "58p0IGxhLXAi"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "version = \"3.5.3\"\n",
        "!wget https://downloads.apache.org/spark/spark-{version}/spark-{version}-bin-hadoop3.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j0mhca5LYrG",
        "outputId": "2f371335-23b4-4382-e110-7a040b079227"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-18 20:46:07--  https://downloads.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.208.237, 2a01:4f8:10a:39da::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400864419 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.3-bin-hadoop3.tgz.1’\n",
            "\n",
            "      spark-3.5.3-b   0%[                    ]   1.18M  1.01MB/s               ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xzvf spark-{version}-bin-hadoop3.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN3h7C-ILbBl",
        "outputId": "76d9e44a-e529-4f9c-bdcd-8f42aeec7d92"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark-3.5.3-bin-hadoop3/\n",
            "spark-3.5.3-bin-hadoop3/data/\n",
            "spark-3.5.3-bin-hadoop3/data/graphx/\n",
            "spark-3.5.3-bin-hadoop3/data/graphx/users.txt\n",
            "spark-3.5.3-bin-hadoop3/data/graphx/followers.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/sample_linear_regression_data.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/sample_fpgrowth.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/sample_libsvm_data.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/gmm_data.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/kmeans_data.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/sample_lda_data.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/pagerank_data.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/sample_movielens_data.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/pic_data.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/sample_binary_classification_data.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/ridge-data/\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/ridge-data/lpsa.data\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/images/\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/images/license.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/images/origin/\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/images/origin/kittens/\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/images/origin/kittens/DP802813.jpg\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/images/origin/kittens/DP153539.jpg\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/images/origin/kittens/54893.jpg\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/images/origin/kittens/not-image.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/images/origin/multi-channel/\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/images/origin/multi-channel/BGRA.png\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/images/origin/license.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/sample_kmeans_data.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/als/\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/als/test.data\n",
            "spark-3.5.3-bin-hadoop3/data/mllib/sample_svm_data.txt\n",
            "spark-3.5.3-bin-hadoop3/data/artifact-tests/\n",
            "spark-3.5.3-bin-hadoop3/data/artifact-tests/smallJar.jar\n",
            "spark-3.5.3-bin-hadoop3/data/artifact-tests/crc/\n",
            "spark-3.5.3-bin-hadoop3/data/artifact-tests/crc/smallJar.txt\n",
            "spark-3.5.3-bin-hadoop3/data/artifact-tests/crc/README.md\n",
            "spark-3.5.3-bin-hadoop3/data/artifact-tests/crc/junitLargeJar.txt\n",
            "spark-3.5.3-bin-hadoop3/data/artifact-tests/junitLargeJar.jar\n",
            "spark-3.5.3-bin-hadoop3/data/streaming/\n",
            "spark-3.5.3-bin-hadoop3/data/streaming/AFINN-111.txt\n",
            "spark-3.5.3-bin-hadoop3/sbin/\n",
            "spark-3.5.3-bin-hadoop3/sbin/start-all.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/start-mesos-shuffle-service.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/start-mesos-dispatcher.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/start-thriftserver.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/spark-config.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/stop-workers.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/stop-master.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/start-slave.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/stop-thriftserver.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/decommission-slave.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/start-connect-server.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/start-worker.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/stop-all.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/start-history-server.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/stop-history-server.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/stop-mesos-dispatcher.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/workers.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/start-slaves.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/stop-slave.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/decommission-worker.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/spark-daemons.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/stop-worker.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/start-workers.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/start-master.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/spark-daemon.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/slaves.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/stop-slaves.sh\n",
            "spark-3.5.3-bin-hadoop3/sbin/stop-connect-server.sh\n",
            "spark-3.5.3-bin-hadoop3/yarn/\n",
            "spark-3.5.3-bin-hadoop3/yarn/spark-3.5.3-yarn-shuffle.jar\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "iiPBQcRWLcHn"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/spark-{version}-bin-hadoop3\""
      ],
      "metadata": {
        "id": "9takgeBfLfLf"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "JBBvi07SLgvb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-qGZ8w6LIdH",
        "outputId": "adfb71d3-3091-4f02-c98e-55f23242213a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing parameters numFeatures=100, numTrees=10, maxDepth=5\n",
            "Testing parameters numFeatures=100, numTrees=10, maxDepth=10\n",
            "Testing parameters numFeatures=100, numTrees=10, maxDepth=20\n",
            "Testing parameters numFeatures=100, numTrees=50, maxDepth=5\n",
            "Testing parameters numFeatures=100, numTrees=50, maxDepth=10\n",
            "Testing parameters numFeatures=100, numTrees=50, maxDepth=20\n",
            "Testing parameters numFeatures=100, numTrees=100, maxDepth=5\n",
            "Testing parameters numFeatures=100, numTrees=100, maxDepth=10\n",
            "Testing parameters numFeatures=100, numTrees=100, maxDepth=20\n",
            "Testing parameters numFeatures=500, numTrees=10, maxDepth=5\n",
            "Testing parameters numFeatures=500, numTrees=10, maxDepth=10\n",
            "Testing parameters numFeatures=500, numTrees=10, maxDepth=20\n",
            "Testing parameters numFeatures=500, numTrees=50, maxDepth=5\n",
            "Testing parameters numFeatures=500, numTrees=50, maxDepth=10\n",
            "Testing parameters numFeatures=500, numTrees=50, maxDepth=20\n",
            "Testing parameters numFeatures=500, numTrees=100, maxDepth=5\n",
            "Testing parameters numFeatures=500, numTrees=100, maxDepth=10\n",
            "Testing parameters numFeatures=500, numTrees=100, maxDepth=20\n",
            "Testing parameters numFeatures=1000, numTrees=10, maxDepth=5\n",
            "Testing parameters numFeatures=1000, numTrees=10, maxDepth=10\n",
            "Testing parameters numFeatures=1000, numTrees=10, maxDepth=20\n",
            "Testing parameters numFeatures=1000, numTrees=50, maxDepth=5\n",
            "Testing parameters numFeatures=1000, numTrees=50, maxDepth=10\n",
            "Testing parameters numFeatures=1000, numTrees=50, maxDepth=20\n",
            "Testing parameters numFeatures=1000, numTrees=100, maxDepth=5\n",
            "Testing parameters numFeatures=1000, numTrees=100, maxDepth=10\n",
            "Testing parameters numFeatures=1000, numTrees=100, maxDepth=20\n",
            "+-----------+--------+--------+------------------+------------------+------------------+------------------+\n",
            "|numFeatures|numTrees|maxDepth|          f1_score|          accuracy|         precision|            recall|\n",
            "+-----------+--------+--------+------------------+------------------+------------------+------------------+\n",
            "|        100|      10|       5|0.8419707403428595|0.8530120481927711|0.8761504542939225|0.8530120481927711|\n",
            "|        100|      10|      10|0.8862691841700209|  0.89143944197844|0.9041309655200702|0.8914394419784402|\n",
            "|        100|      10|      20|0.9295631913665057|0.9312618896639189|0.9359541693341138|0.9312618896639189|\n",
            "|        100|      50|       5|0.8319429615069518|0.8447685478757134|0.8710194169144005|0.8447685478757133|\n",
            "|        100|      50|      10|0.8934411891992562|0.8979074191502854|0.9092003182086714|0.8979074191502854|\n",
            "|        100|      50|      20|0.9283537735942333|0.9301204819277108| 0.934988028504401|0.9301204819277109|\n",
            "|        100|     100|       5|0.8379074309170542| 0.849714648065948|0.8744176526019333| 0.849714648065948|\n",
            "|        100|     100|      10|0.8986299375022448|0.9025998731769181|0.9127765295898227|0.9025998731769183|\n",
            "|        100|     100|      20|0.9302429548718754|0.9318960050729233|0.9364387910919791|0.9318960050729233|\n",
            "|        500|      10|       5|0.7632546907913371|0.7922637920101459|0.8424527081959057|0.7922637920101459|\n",
            "|        500|      10|      10|  0.86714101291761|0.8748256182625238|0.8933981955006693|0.8748256182625237|\n",
            "|        500|      10|      20|0.9229295282293215|0.9250475586556753|0.9309100716495624|0.9250475586556753|\n",
            "|        500|      50|       5|0.7752739606842083|0.8010145846544071|0.8470648012849773| 0.801014584654407|\n",
            "|        500|      50|      10|0.8900904646662845| 0.894990488268865|0.9075904087919191| 0.894990488268865|\n",
            "|        500|      50|      20| 0.915021381358216|0.9176918199112238|0.9249900009622596|0.9176918199112238|\n",
            "|        500|     100|       5|0.7396276174203005|0.7755231452124287|  0.83329892432745|0.7755231452124287|\n",
            "|        500|     100|      10|0.8819156810889247|0.8876347495244135|0.9017297990768547|0.8876347495244135|\n",
            "|        500|     100|      20|0.9172156322350478|0.9197209892200381|0.9265789570870928|0.9197209892200381|\n",
            "|       1000|      10|       5|0.7742549502422094|0.8003804692454026|0.8474095279802831|0.8003804692454026|\n",
            "|       1000|      10|      10|0.8278677142963999|0.8417247939124921|0.8706713558133944|0.8417247939124921|\n",
            "+-----------+--------+--------+------------------+------------------+------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.functions import col, udf, length, desc\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, Tokenizer, StopWordsRemover, HashingTF, IDF, VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"bot_classifier\").getOrCreate()\n",
        "\n",
        "df = spark.read.json(\"edits.json\", multiLine=False)\n",
        "\n",
        "df = df.select(\"type\", \"namespace\", \"comment\", \"bot\", \"length\")\n",
        "\n",
        "\n",
        "def calculate_length_udf(length):\n",
        "    if isinstance(length, dict):\n",
        "        return length.get('new', 0) - length.get('old', 0)\n",
        "    return 0\n",
        "\n",
        "\n",
        "length_udf = udf(calculate_length_udf, IntegerType())\n",
        "df = df.withColumn(\"length_diff\", length_udf(col(\"length\")))\n",
        "\n",
        "df = df.withColumn(\"comment_length\", length(col(\"comment\")))\n",
        "\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "\n",
        "def create_pipeline(numFeatures, numTrees, maxDepth):\n",
        "  type_indexer = StringIndexer(inputCol=\"type\", outputCol=\"type_index\", handleInvalid=\"keep\")\n",
        "  namespace_indexer = StringIndexer(inputCol=\"namespace\", outputCol=\"namespace_index\", handleInvalid=\"keep\")\n",
        "  type_encoder = OneHotEncoder(inputCol=\"type_index\", outputCol=\"type_encoded\")\n",
        "  namespace_encoder = OneHotEncoder(inputCol=\"namespace_index\", outputCol=\"namespace_encoded\")\n",
        "\n",
        "  tokenizer = Tokenizer(inputCol=\"comment\", outputCol=\"words\")\n",
        "  stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "  hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=numFeatures)\n",
        "  idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
        "\n",
        "  assembler = VectorAssembler(\n",
        "      inputCols=[\"length_diff\", \"comment_length\", \"type_encoded\", \"namespace_encoded\", \"tfidf_features\"],\n",
        "      outputCol=\"features\"\n",
        "  )\n",
        "\n",
        "  rf = RandomForestClassifier(labelCol=\"bot\", featuresCol=\"features\", numTrees=numTrees, maxDepth=maxDepth)\n",
        "\n",
        "  pipeline = Pipeline(stages=[\n",
        "      type_indexer, namespace_indexer,\n",
        "      type_encoder, namespace_encoder,\n",
        "      tokenizer, stopwords_remover, hashing_tf, idf,\n",
        "      assembler, rf\n",
        "  ])\n",
        "  return pipeline\n",
        "\n",
        "\n",
        "def train_and_evaluate(train, test, numFeatures, numTrees, maxDepth):\n",
        "  pipeline = create_pipeline(numFeatures, numTrees, maxDepth)\n",
        "\n",
        "  model = pipeline.fit(train)\n",
        "  predictions = model.transform(test)\n",
        "\n",
        "  evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"bot\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "  evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"bot\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "  evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"bot\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "  evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"bot\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "\n",
        "  f1_score = evaluator_f1.evaluate(predictions)\n",
        "  accuracy = evaluator_accuracy.evaluate(predictions)\n",
        "  precision = evaluator_precision.evaluate(predictions)\n",
        "  recall = evaluator_recall.evaluate(predictions)\n",
        "\n",
        "  return model, f1_score, accuracy, precision, recall\n",
        "\n",
        "params = {\n",
        "    \"numFeatures\": [100, 500, 1000],\n",
        "    \"numTrees\": [10, 50, 100],\n",
        "    \"maxDepth\": [5, 10, 20]\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for numFeatures, numTrees, maxDepth in product(params[\"numFeatures\"], params[\"numTrees\"], params[\"maxDepth\"]):\n",
        "  print(f\"Testing parameters numFeatures={numFeatures}, numTrees={numTrees}, maxDepth={maxDepth}\")\n",
        "\n",
        "  _, f1_score, accuracy, precision, recall = train_and_evaluate(train, test, numFeatures, numTrees, maxDepth)\n",
        "  results.append(Row(numFeatures=numFeatures, numTrees=numTrees, maxDepth=maxDepth, f1_score=f1_score, accuracy=accuracy, precision=precision, recall=recall))\n",
        "\n",
        "results_df = spark.createDataFrame(results)\n",
        "best_result = results_df.orderBy(desc(\"f1_score\")).first()\n",
        "\n",
        "results_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best parameters: numFeatures={best_result.numFeatures}, numTrees={best_result.numTrees}, maxDepth={best_result.maxDepth}\")\n",
        "print(f\"F1 Score: {f1_score};\\tAccuracy: {accuracy};\\tPrecision: {precision};\\tRecall: {recall}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp2NW-Xporo3",
        "outputId": "abd13bc6-2a57-4b60-8195-3ad8d7c04fe7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: numFeatures=100, numTrees=100, maxDepth=20\n",
            "F1 Score: 0.911711785200636;\tAccuracy: 0.9146480659480025;\tPrecision: 0.9226667180813601;\tRecall: 0.9146480659480025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, f1_score, accuracy, precision, recall = train_and_evaluate(train, test, best_result.numFeatures, best_result.numTrees, best_result.maxDepth)\n",
        "\n",
        "best_model_path = \"bot_classifier\"\n",
        "model.write().overwrite().save(best_model_path)"
      ],
      "metadata": {
        "id": "-3uf-gYsLj25"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import PipelineModel\n",
        "\n",
        "\n",
        "loaded_model = PipelineModel.load(best_model_path)"
      ],
      "metadata": {
        "id": "X2EEt2gzl-KX"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KgCQXE8jz2ZA"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}